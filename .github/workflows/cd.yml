name: CD

on:
  push:
    branches: [ "main" ]
    paths:
      - 'src/**'
      - 'k8s/**'
      - '.github/workflows/cd.yml'
  workflow_dispatch:

env:
  ECR_REPOSITORY: usuarios-service
  IMAGE_TAG: latest

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build and Push Docker Image
        id: build-push
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          set -e
          
          if [ -z "$ECR_REGISTRY" ]; then
            echo "ERRO: ECR_REGISTRY n√£o est√° definido"
            exit 1
          fi
          
          DOCKER_IMAGE_FULL="$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG"
          echo "Building image: $DOCKER_IMAGE_FULL"
          
          docker build -t $DOCKER_IMAGE_FULL .
          
          echo "Pushing image to ECR..."
          docker push $DOCKER_IMAGE_FULL
          
          echo "DOCKER_IMAGE_FULL=$DOCKER_IMAGE_FULL" >> $GITHUB_OUTPUT
          echo "Image $DOCKER_IMAGE_FULL pushed successfully"

      - name: Prepare K8s manifests
        env:
          DOCKER_IMAGE_FULL: ${{ steps.build-push.outputs.DOCKER_IMAGE_FULL }}
          DATABASE_POSTGRES_CONNECTION_STRING: ${{ secrets.USUARIOS_POSTGRES_CONNECTION }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
          JWT_KEY: ${{ secrets.JWT_KEY }}
          JWT_EXPIRATION_TIME_HOUR: ${{ secrets.JWT_EXPIRATION_TIME_HOUR }}
          JWT_INCREASE_EXPIRATION_TIME_MINUTES: ${{ secrets.JWT_INCREASE_EXPIRATION_TIME_MINUTES }}
        run: |
          set -e
          
          echo "Validando vari√°veis obrigat√≥rias..."
          if [ -z "$DOCKER_IMAGE_FULL" ]; then
            echo "ERRO: DOCKER_IMAGE_FULL n√£o est√° definido"
            exit 1
          fi
          if [ -z "$DATABASE_POSTGRES_CONNECTION_STRING" ]; then
            echo "ERRO: DATABASE_POSTGRES_CONNECTION_STRING n√£o est√° definido"
            exit 1
          fi
          if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ] || [ -z "$AWS_SESSION_TOKEN" ]; then
            echo "ERRO: Credenciais AWS n√£o est√£o definidas"
            exit 1
          fi
          if [ -z "$AWS_ACCOUNT_ID" ]; then
            echo "ERRO: AWS_ACCOUNT_ID n√£o est√° definido"
            exit 1
          fi
          
          mkdir -p k8s/usuarios/processed
          export DOCKER_IMAGE_FULL DATABASE_POSTGRES_CONNECTION_STRING AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_ACCOUNT_ID JWT_KEY JWT_EXPIRATION_TIME_HOUR JWT_INCREASE_EXPIRATION_TIME_MINUTES
          
          echo "Processando arquivos YAML..."
          if command -v envsubst >/dev/null 2>&1; then
            envsubst < k8s/configmap.yaml > k8s/usuarios/processed/configmap.yaml
            envsubst < k8s/secret.yaml > k8s/usuarios/processed/secret.yaml
            envsubst < k8s/deployment.yaml > k8s/usuarios/processed/deployment.yaml
          else
            sed -e "s|\${DOCKER_IMAGE_FULL}|${DOCKER_IMAGE_FULL}|g" \
                -e "s|\${DATABASE_POSTGRES_CONNECTION_STRING}|${DATABASE_POSTGRES_CONNECTION_STRING}|g" \
                -e "s|\${AWS_ACCESS_KEY_ID}|${AWS_ACCESS_KEY_ID}|g" \
                -e "s|\${AWS_SECRET_ACCESS_KEY}|${AWS_SECRET_ACCESS_KEY}|g" \
                -e "s|\${AWS_SESSION_TOKEN}|${AWS_SESSION_TOKEN}|g" \
                -e "s|\${AWS_ACCOUNT_ID}|${AWS_ACCOUNT_ID}|g" \
                -e "s|\${JWT_KEY}|${JWT_KEY}|g" \
                -e "s|\${JWT_EXPIRATION_TIME_HOUR}|${JWT_EXPIRATION_TIME_HOUR}|g" \
                -e "s|\${JWT_INCREASE_EXPIRATION_TIME_MINUTES}|${JWT_INCREASE_EXPIRATION_TIME_MINUTES}|g" \
                k8s/configmap.yaml > k8s/usuarios/processed/configmap.yaml
            sed -e "s|\${DATABASE_POSTGRES_CONNECTION_STRING}|${DATABASE_POSTGRES_CONNECTION_STRING}|g" \
                -e "s|\${AWS_ACCESS_KEY_ID}|${AWS_ACCESS_KEY_ID}|g" \
                -e "s|\${AWS_SECRET_ACCESS_KEY}|${AWS_SECRET_ACCESS_KEY}|g" \
                -e "s|\${AWS_SESSION_TOKEN}|${AWS_SESSION_TOKEN}|g" \
                -e "s|\${JWT_KEY}|${JWT_KEY}|g" \
                -e "s|\${JWT_EXPIRATION_TIME_HOUR}|${JWT_EXPIRATION_TIME_HOUR}|g" \
                -e "s|\${JWT_INCREASE_EXPIRATION_TIME_MINUTES}|${JWT_INCREASE_EXPIRATION_TIME_MINUTES}|g" \
                k8s/secret.yaml > k8s/usuarios/processed/secret.yaml
            sed -e "s|\${DOCKER_IMAGE_FULL}|${DOCKER_IMAGE_FULL}|g" \
                k8s/deployment.yaml > k8s/usuarios/processed/deployment.yaml
          fi
          
          cp k8s/service.yaml k8s/usuarios/processed/service.yaml
          cp k8s/hpa.yaml k8s/usuarios/processed/hpa.yaml
          if [ -f k8s/ingress.yaml ]; then
            cp k8s/ingress.yaml k8s/usuarios/processed/ingress.yaml
          fi
          if [ -f k8s/middleware.yaml ]; then
            cp k8s/middleware.yaml k8s/usuarios/processed/middleware.yaml
          fi
          if [ -f k8s/namespace.yaml ]; then
            cp k8s/namespace.yaml k8s/usuarios/processed/namespace.yaml
          else
            echo "Aviso: namespace.yaml n√£o encontrado, ser√° criado automaticamente"
          fi
          
          if [ -f k8s/traefik/service.yaml ]; then
            echo "üìã Copiando traefik/service.yaml..."
            cp k8s/traefik/service.yaml k8s/usuarios/processed/traefik-service.yaml
            if [ ! -f k8s/usuarios/processed/traefik-service.yaml ]; then
              echo "‚ùå ERRO: Falha ao copiar traefik/service.yaml"
              exit 1
            fi
            echo "‚úÖ traefik-service.yaml copiado com sucesso"
          else
            echo "‚ö†Ô∏è  AVISO: k8s/traefik/service.yaml n√£o encontrado"
          fi
          
          if [ -f k8s/traefik/service-backup.yaml ]; then
            echo "üìã Copiando traefik/service-backup.yaml..."
            cp k8s/traefik/service-backup.yaml k8s/usuarios/processed/traefik-service-backup.yaml
            echo "‚úÖ traefik-service-backup.yaml copiado com sucesso"
          fi
          
          if grep -r '\${' k8s/usuarios/processed/*.yaml 2>/dev/null; then
            echo "ERRO: Vari√°veis n√£o substitu√≠das encontradas"
            exit 1
          fi

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Configure kubectl for EKS
        run: |
          aws eks update-kubeconfig \
            --name ${{ secrets.EKS_CLUSTER_NAME }} \
            --region us-east-1

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version

      - name: Install Traefik
        run: |
          echo "Verificando se o Traefik j√° est√° instalado..."
          
          if kubectl get deployment traefik -n kube-system >/dev/null 2>&1 || \
             kubectl get daemonset traefik -n kube-system >/dev/null 2>&1 || \
             kubectl get deployment traefik -n traefik >/dev/null 2>&1; then
            echo "‚úÖ Traefik j√° est√° instalado no cluster."
            kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik 2>/dev/null || \
            kubectl get pods -n traefik -l app.kubernetes.io/name=traefik 2>/dev/null || true
          else
            echo "üì¶ Traefik n√£o encontrado. Instalando via Helm..."
            
            helm repo add traefik https://traefik.github.io/charts
            helm repo update
            
            echo "Instalando Traefik no namespace kube-system..."
            helm upgrade --install traefik traefik/traefik \
              --namespace kube-system \
              --create-namespace \
              --set service.type=ClusterIP \
              --set ports.web.port=80 \
              --set ports.websecure.port=443 \
              --set additionalArguments[0]=--api.dashboard=false \
              --set additionalArguments[1]=--api.insecure=false \
              --wait \
              --timeout 5m || {
                echo "‚ö†Ô∏è  Erro ao instalar Traefik. Verificando status..."
                kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik || true
                exit 1
              }
            
            echo "‚úÖ Traefik instalado com sucesso!"
            echo "‚è≥ Aguardando pods ficarem prontos..."
            sleep 15
            
            echo "Status do Traefik:"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik || true
          fi

      - name: Verify AWS Load Balancer Controller
        run: |
          echo "üîç Verificando AWS Load Balancer Controller..."
          
          if kubectl get deployment -n kube-system aws-load-balancer-controller >/dev/null 2>&1; then
            echo "‚úÖ AWS Load Balancer Controller encontrado"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          else
            echo "‚ö†Ô∏è  AWS Load Balancer Controller N√ÉO encontrado"
            echo ""
            echo "O EKS moderno (1.24+) usa o controlador de servi√ßo integrado."
            echo "Se o Load Balancer n√£o for criado, verifique:"
            echo "1. Permiss√µes IAM do cluster (IRSA ou role do node group)"
            echo "2. Tags das sub-redes (kubernetes.io/role/elb=1 para p√∫blicas)"
            echo "3. Logs do controlador: kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller"
          fi

      - name: Setup Traefik CRDs
        run: |
          echo "Verificando se o Traefik est√° instalado no cluster..."
          
          if kubectl get deployment traefik -n kube-system >/dev/null 2>&1 || \
             kubectl get daemonset traefik -n kube-system >/dev/null 2>&1 || \
             kubectl get deployment traefik -n traefik >/dev/null 2>&1; then
            echo "Traefik encontrado no cluster."
            
            if ! kubectl get crd middlewares.traefik.containo.us >/dev/null 2>&1; then
              echo "CRDs do Traefik n√£o encontrados. Tentando instalar..."
              
              kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.11/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml 2>/dev/null || \
              kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v3.0/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml 2>/dev/null || \
              echo "‚ö†Ô∏è  N√£o foi poss√≠vel instalar os CRDs automaticamente."
              
              echo "Aguardando CRDs ficarem dispon√≠veis..."
              sleep 10
            else
              echo "‚úÖ CRDs do Traefik j√° est√£o instalados."
            fi
          else
            echo "‚ö†Ô∏è  Traefik n√£o encontrado no cluster. O middleware ser√° ignorado."
            echo "Para usar o middleware, instale o Traefik primeiro ou use outro Ingress Controller."
          fi

      - name: Deploy to EKS
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          AWS_REGION: us-east-1
        run: |
          set -e
          
          # Criar namespace se n√£o existir
          if [ -f k8s/usuarios/processed/namespace.yaml ]; then
            kubectl apply -f k8s/usuarios/processed/namespace.yaml
          else
            kubectl create namespace users --dry-run=client -o yaml | kubectl apply -f -
          fi
          
          # Aguardar namespace estar dispon√≠vel (evita condi√ß√£o de corrida)
          echo "‚è≥ Aguardando namespace 'users' estar dispon√≠vel..."
          for i in {1..30}; do
            if kubectl get namespace users >/dev/null 2>&1; then
              echo "‚úÖ Namespace 'users' est√° dispon√≠vel"
              break
            else
              if [ $i -eq 30 ]; then
                echo "‚ùå ERRO: Namespace 'users' n√£o est√° dispon√≠vel ap√≥s 30 tentativas"
                exit 1
              fi
              echo "‚è≥ Aguardando namespace... ($i/30)"
              sleep 1
            fi
          done
          
          if [ -z "$ECR_REGISTRY" ]; then
            ECR_REGISTRY="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          fi
          
          kubectl delete secret ecr-secret -n users 2>/dev/null || true
          
          TOKEN=$(aws ecr get-login-password --region $AWS_REGION) || exit 1
          kubectl create secret docker-registry ecr-secret \
            --docker-server="$ECR_REGISTRY" \
            --docker-username=AWS \
            --docker-password="$TOKEN" \
            --namespace=users || exit 1
          
          CURRENT_IMAGE=$(kubectl get deployment usuarios-deployment -n users -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "")
          NEW_IMAGE=$(grep "image:" k8s/usuarios/processed/deployment.yaml | awk '{print $2}' | tr -d '"')
          
          CONFIGMAP_CHANGED=false
          SECRET_CHANGED=false
          
          if kubectl get configmap usuarios-config -n users >/dev/null 2>&1; then
            OLD_CONFIGMAP=$(kubectl get configmap usuarios-config -n users -o yaml | grep -A 100 "^data:" | sort)
            kubectl apply -f k8s/usuarios/processed/configmap.yaml >/dev/null 2>&1
            NEW_CONFIGMAP=$(kubectl get configmap usuarios-config -n users -o yaml | grep -A 100 "^data:" | sort)
            if [ "$OLD_CONFIGMAP" != "$NEW_CONFIGMAP" ]; then
              CONFIGMAP_CHANGED=true
              echo "üîÑ ConfigMap mudou. Deployment ser√° reiniciado."
            fi
          else
            kubectl apply -f k8s/usuarios/processed/configmap.yaml >/dev/null 2>&1
            CONFIGMAP_CHANGED=true
            echo "üÜï ConfigMap criado. Deployment ser√° reiniciado."
          fi
          
          if kubectl get secret usuarios-secret -n users >/dev/null 2>&1; then
            OLD_SECRET=$(kubectl get secret usuarios-secret -n users -o yaml | grep -A 100 "^data:" | sort)
            kubectl apply -f k8s/usuarios/processed/secret.yaml >/dev/null 2>&1
            NEW_SECRET=$(kubectl get secret usuarios-secret -n users -o yaml | grep -A 100 "^data:" | sort)
            if [ "$OLD_SECRET" != "$NEW_SECRET" ]; then
              SECRET_CHANGED=true
              echo "üîÑ Secret mudou. Deployment ser√° reiniciado."
            fi
          else
            kubectl apply -f k8s/usuarios/processed/secret.yaml >/dev/null 2>&1
            SECRET_CHANGED=true
            echo "üÜï Secret criado. Deployment ser√° reiniciado."
          fi
          
          kubectl apply -f k8s/usuarios/processed/deployment.yaml
          
          kubectl apply -f k8s/usuarios/processed/service.yaml
          kubectl apply -f k8s/usuarios/processed/hpa.yaml
          
          if [ -f k8s/usuarios/processed/middleware.yaml ]; then
            if kubectl get crd middlewares.traefik.containo.us >/dev/null 2>&1; then

              kubectl delete middleware stripprefix -n users --ignore-not-found
              kubectl delete middleware usuarios-stripprefix -n users --ignore-not-found
  
              echo "‚úÖ Aplicando middleware do Traefik..."
              if kubectl apply -f k8s/usuarios/processed/middleware.yaml; then
                echo "‚úÖ Middleware aplicado com sucesso!"
                echo "‚è≥ Aguardando middleware ficar dispon√≠vel..."
                sleep 5
                
                if kubectl get middleware usuarios-strip -n users >/dev/null 2>&1; then
                  echo "‚úÖ Middleware usuarios-strip confirmado no cluster"
                else
                  echo "‚ö†Ô∏è  Middleware n√£o encontrado ap√≥s aplica√ß√£o. Verificando..."
                  kubectl get middleware -n users
                fi
              else
                echo "‚ö†Ô∏è  Erro ao aplicar middleware. Continuando sem ele..."
              fi
            else
              echo "‚ö†Ô∏è  CRDs do Traefik n√£o est√£o dispon√≠veis. Pulando aplica√ß√£o do middleware."
              echo "O Ingress funcionar√°, mas sem o stripPrefix. Configure o path corretamente no Ingress."
            fi
          fi
          
          if [ -f k8s/usuarios/processed/ingress.yaml ]; then
            echo "Aplicando Ingress..."
            if kubectl apply -f k8s/usuarios/processed/ingress.yaml; then
              echo "‚úÖ Ingress aplicado com sucesso!"
              echo "‚è≥ Aguardando Traefik processar o Ingress..."
              sleep 5
            else
              echo "‚ö†Ô∏è  Erro ao aplicar Ingress. Verifique se o Ingress Controller est√° instalado."
            fi
          fi
          
          if [ -f k8s/usuarios/processed/traefik-service.yaml ]; then
            echo "üîç Verificando se o Traefik Service j√° existe..."
            EXISTING_SVC=$(kubectl get svc traefik-loadbalancer -n kube-system 2>/dev/null || echo "")
            
            NEED_APPLY=false
            if [ -n "$EXISTING_SVC" ]; then
              CURRENT_SCHEME=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.metadata.annotations.service\.beta\.kubernetes\.io/aws-load-balancer-scheme}' 2>/dev/null || echo "")
              NLB_HOSTNAME=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
              
              if [ "$CURRENT_SCHEME" = "internet-facing" ] && [ -n "$NLB_HOSTNAME" ]; then
                echo "‚úÖ Traefik Service j√° existe, est√° configurado como internet-facing e o LoadBalancer foi provisionado."
                echo "   Hostname do LoadBalancer: $NLB_HOSTNAME"
                echo "‚ÑπÔ∏è  O LoadBalancer do Traefik √© compartilhado - n√£o precisa ser criado por cada projeto."
                NEED_APPLY=false
              elif [ "$CURRENT_SCHEME" = "internet-facing" ] && [ -z "$NLB_HOSTNAME" ]; then
                echo "‚ö†Ô∏è  Traefik Service existe e est√° configurado como internet-facing, mas o LoadBalancer ainda n√£o foi provisionado."
                echo "   Aguardando provisionamento do LoadBalancer..."
                NEED_APPLY=false
              else
                echo "‚ö†Ô∏è  Traefik Service existe mas com scheme incorreto ($CURRENT_SCHEME). For√ßando atualiza√ß√£o para internet-facing..."
                echo "   ‚ö†Ô∏è  IMPORTANTE: Se o LoadBalancer j√° foi criado como internal, ser√° necess√°rio delet√°-lo e recriar."
                echo "   Aplicando configura√ß√£o correta (internet-facing)..."
                NEED_APPLY=true
              fi
            else
              echo "üì¶ Traefik Service n√£o existe. Criando novo (ser√° compartilhado por todos os projetos)..."
              echo "üîç Verificando se os pods do Traefik est√£o dispon√≠veis..."
              TRAEFIK_PODS=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
              if [ -z "$TRAEFIK_PODS" ]; then
                echo "‚ö†Ô∏è  AVISO: Nenhum pod do Traefik encontrado com label app.kubernetes.io/name=traefik"
                echo "Verificando pods do Traefik:"
                kubectl get pods -n kube-system | grep traefik || echo "Nenhum pod encontrado"
              else
                echo "‚úÖ Pods do Traefik encontrados: $TRAEFIK_PODS"
              fi
              NEED_APPLY=true
            fi
            
            if [ "$NEED_APPLY" = "true" ]; then
              # Se o scheme est√° incorreto e j√° existe um LoadBalancer, pode ser necess√°rio deletar o servi√ßo primeiro
              if [ -n "$EXISTING_SVC" ] && [ "$CURRENT_SCHEME" != "internet-facing" ] && [ -n "$NLB_HOSTNAME" ]; then
                echo "‚ö†Ô∏è  LoadBalancer existente com scheme incorreto detectado."
                echo "   O AWS Load Balancer Controller n√£o pode alterar o scheme de um LoadBalancer existente."
                echo "   Ser√° necess√°rio deletar o servi√ßo e recriar com a configura√ß√£o correta."
                echo "   ‚ö†Ô∏è  Isso causar√° uma breve interrup√ß√£o no servi√ßo."
                echo ""
                echo "üóëÔ∏è  Deletando servi√ßo antigo para recriar com internet-facing..."
                kubectl delete svc traefik-loadbalancer -n kube-system --ignore-not-found=true
                echo "‚è≥ Aguardando 10 segundos para a AWS remover o LoadBalancer antigo..."
                sleep 10
              fi
              
              echo "‚úÖ Aplicando Traefik Service LoadBalancer (internet-facing)..."
              
              # Verificar AWS Load Balancer Controller antes de aplicar
              echo "üîç Verificando AWS Load Balancer Controller..."
              if kubectl get deployment -n kube-system aws-load-balancer-controller >/dev/null 2>&1; then
                echo "‚úÖ AWS Load Balancer Controller encontrado"
                CONTROLLER_POD=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
                if [ -n "$CONTROLLER_POD" ]; then
                  echo "   Pod do Controller: $CONTROLLER_POD"
                  CONTROLLER_READY=$(kubectl get pod $CONTROLLER_POD -n kube-system -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "")
                  if [ "$CONTROLLER_READY" = "True" ]; then
                    echo "   ‚úÖ Controller est√° pronto"
                  else
                    echo "   ‚ö†Ô∏è  Controller n√£o est√° pronto. Status:"
                    kubectl get pod $CONTROLLER_POD -n kube-system
                  fi
                else
                  echo "   ‚ö†Ô∏è  Nenhum pod do Controller encontrado"
                fi
              else
                echo "   ‚ö†Ô∏è  AWS Load Balancer Controller N√ÉO encontrado!"
                echo "   O LoadBalancer n√£o ser√° provisionado sem o Controller."
              fi
              
              if kubectl apply -f k8s/usuarios/processed/traefik-service.yaml; then
                echo "‚úÖ Traefik Service aplicado com sucesso!"
                
                echo "üìã Verificando detalhes do service..."
                kubectl get svc traefik-loadbalancer -n kube-system -o yaml | head -50
                
                # Verificar se o tipo est√° correto
                SERVICE_TYPE=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.spec.type}' 2>/dev/null || echo "")
                echo ""
                echo "üìä Tipo do Service: $SERVICE_TYPE"
                if [ "$SERVICE_TYPE" != "LoadBalancer" ]; then
                  echo "   ‚ö†Ô∏è  ERRO: Service n√£o est√° configurado como tipo LoadBalancer!"
                  echo "   Tipo atual: $SERVICE_TYPE"
                fi
                
                # Verificar anota√ß√µes
                echo ""
                echo "üìã Anota√ß√µes do Service:"
                kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.metadata.annotations}' | jq '.' 2>/dev/null || kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.metadata.annotations}'
                echo ""
                
                echo "‚è≥ Aguardando LoadBalancer ser provisionado (pode levar at√© 3 minutos)..."
                for i in {1..18}; do
                  sleep 10
                  NLB_HOSTNAME=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
                  if [ -n "$NLB_HOSTNAME" ]; then
                    echo "‚úÖ LoadBalancer criado! Hostname: $NLB_HOSTNAME"
                    break
                  else
                    echo "‚è≥ Aguardando... ($i/18)"
                    # A cada 6 tentativas, mostrar eventos
                    if [ $((i % 6)) -eq 0 ]; then
                      echo "   Verificando eventos recentes..."
                      kubectl get events -n kube-system --sort-by='.lastTimestamp' | grep -i "traefik\|loadbalancer" | tail -5 || echo "   Nenhum evento relevante"
                    fi
                  fi
                done
                
                if [ -z "$NLB_HOSTNAME" ]; then
                  echo ""
                  echo "‚ùå LoadBalancer N√ÉO foi provisionado ap√≥s 3 minutos."
                  echo ""
                  echo "üìä Status atual do Service:"
                  kubectl get svc traefik-loadbalancer -n kube-system -o wide
                  echo ""
                  echo "üìã Eventos relacionados:"
                  kubectl get events -n kube-system --sort-by='.lastTimestamp' | grep -i "traefik\|loadbalancer" | tail -15 || echo "Nenhum evento encontrado"
                  echo ""
                  
                  # Verificar logs do AWS Load Balancer Controller
                  if [ -n "$CONTROLLER_POD" ]; then
                    echo "üìã √öltimos logs do AWS Load Balancer Controller:"
                    kubectl logs $CONTROLLER_POD -n kube-system --tail=20 | grep -i "traefik\|error\|failed" || kubectl logs $CONTROLLER_POD -n kube-system --tail=10
                    echo ""
                  fi
                  
                  echo "üîç Diagn√≥stico - Poss√≠veis causas:"
                  echo "1. ‚ùå AWS Load Balancer Controller n√£o instalado ou n√£o funcionando"
                  echo "2. ‚ùå Permiss√µes IAM insuficientes no cluster EKS (service account)"
                  echo "3. ‚ùå Sub-redes sem tags corretas:"
                  echo "   - kubernetes.io/role/elb=1 (para internet-facing)"
                  echo "   - kubernetes.io/cluster/<cluster-name>=shared"
                  echo "4. ‚ùå Limite de Load Balancers na conta AWS atingido"
                  echo "5. ‚ùå Anota√ß√µes incorretas no Service"
                  echo "6. ‚ùå Service n√£o est√° com type: LoadBalancer"
                  echo ""
                  echo "üí° Para verificar na AWS Console:"
                  echo "   - EC2 > Load Balancers (verificar se aparece algum NLB)"
                  echo "   - Verificar se h√° erros no CloudWatch Logs do Controller"
                fi
              else
                echo "‚ö†Ô∏è  Erro ao aplicar traefik-service.yaml. Tentando service-backup.yaml..."
                if [ -f k8s/usuarios/processed/traefik-service-backup.yaml ]; then
                  kubectl apply -f k8s/usuarios/processed/traefik-service-backup.yaml
                else
                  echo "‚ùå ERRO: N√£o foi poss√≠vel aplicar o Traefik Service!"
                  exit 1
                fi
              fi
              
              NODE_PORT=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.spec.ports[?(@.name=="http")].nodePort}' 2>/dev/null || echo "")
              if [ -n "$NODE_PORT" ]; then
                echo "‚úÖ NodePort atribu√≠do: $NODE_PORT"
                echo "‚ö†Ô∏è  IMPORTANTE: Configure o Target Group do API Gateway para usar a porta $NODE_PORT"
              else
                echo "‚ö†Ô∏è  NodePort ainda n√£o foi atribu√≠do."
              fi
            fi
          elif [ -f k8s/usuarios/processed/traefik-service-backup.yaml ]; then
            echo "‚úÖ Aplicando Traefik Service (backup)..."
            kubectl apply -f k8s/usuarios/processed/traefik-service-backup.yaml
          else
            echo "‚ö†Ô∏è  AVISO: Arquivo traefik-service.yaml n√£o encontrado. O LoadBalancer n√£o ser√° criado."
            echo "Verifique se o arquivo k8s/traefik/service.yaml existe no reposit√≥rio."
          fi
          
          SKIP_ROLLOUT=false
          if [ -n "$CURRENT_IMAGE" ] && [ "$CURRENT_IMAGE" = "$NEW_IMAGE" ] && [ "$CONFIGMAP_CHANGED" = "false" ] && [ "$SECRET_CHANGED" = "false" ]; then
            READY=$(kubectl get deployment usuarios-deployment -n users -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
            DESIRED=$(kubectl get deployment usuarios-deployment -n users -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "1")
            if [ "$READY" = "$DESIRED" ] && [ "$READY" != "0" ]; then
              echo "‚ÑπÔ∏è  Deployment j√° est√° pronto. Nenhuma mudan√ßa detectada."
              SKIP_ROLLOUT=true
            fi
          fi
          
          if [ "$SKIP_ROLLOUT" = "false" ]; then
            if [ -n "$CURRENT_IMAGE" ] && [ "$CURRENT_IMAGE" != "$NEW_IMAGE" ]; then
              echo "üîÑ Imagem mudou. Reiniciando deployment..."
              kubectl rollout restart deployment/usuarios-deployment -n users
            elif [ "$CONFIGMAP_CHANGED" = "true" ] || [ "$SECRET_CHANGED" = "true" ]; then
              echo "üîÑ ConfigMap ou Secret mudou. Reiniciando deployment para aplicar mudan√ßas..."
              kubectl rollout restart deployment/usuarios-deployment -n users
            else
              echo "üîÑ Aguardando deployment ficar pronto..."
            fi
          fi
          
          if ! kubectl rollout status deployment/usuarios-deployment -n users --timeout=300s; then
            echo "ERRO: Rollout falhou ou timeout"
            echo ""
            echo "=== Status dos pods ==="
            kubectl get pods -n users -o wide
            echo ""
            echo "=== Descri√ß√£o do deployment ==="
            kubectl describe deployment usuarios-deployment -n users
            echo ""
            echo "=== Logs dos pods (√∫ltimos 50 linhas) ==="
            for pod in $(kubectl get pods -n users -l app=usuarios -o jsonpath='{.items[*].metadata.name}'); do
              echo "--- Logs do pod: $pod ---"
              kubectl logs $pod -n users --tail=50 || echo "N√£o foi poss√≠vel obter logs do pod $pod"
              echo ""
              echo "--- Descri√ß√£o do pod: $pod ---"
              kubectl describe pod $pod -n users | tail -30
              echo ""
            done
            echo ""
            echo "=== Eventos do namespace ==="
            kubectl get events -n users --sort-by='.lastTimestamp' | tail -20
            exit 1
          fi
          
          echo "Deploy conclu√≠do com sucesso!"

      - name: Verify Traefik and Load Balancer
        run: |
          echo "üîç Verificando status do Traefik e Load Balancer..."
          sleep 10
          
          echo ""
          echo "=== 1. TRAEFIK EST√Å VIVO? ==="
          TRAEFIK_DEPLOYMENT=$(kubectl get deployment traefik -n kube-system 2>/dev/null || echo "")
          TRAEFIK_DAEMONSET=$(kubectl get daemonset traefik -n kube-system 2>/dev/null || echo "")
          
          if [ -n "$TRAEFIK_DEPLOYMENT" ]; then
            echo "‚úÖ Traefik Deployment encontrado:"
            kubectl get deployment traefik -n kube-system
            echo ""
            echo "Pods do Traefik:"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik
            READY_PODS=$(kubectl get deployment traefik -n kube-system -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
            DESIRED_PODS=$(kubectl get deployment traefik -n kube-system -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")
            if [ "$READY_PODS" = "$DESIRED_PODS" ] && [ "$READY_PODS" != "0" ]; then
              echo "‚úÖ Traefik est√° RODANDO ($READY_PODS/$DESIRED_PODS pods prontos)"
            else
              echo "‚ö†Ô∏è  Traefik N√ÉO est√° totalmente pronto ($READY_PODS/$DESIRED_PODS pods)"
            fi
          elif [ -n "$TRAEFIK_DAEMONSET" ]; then
            echo "‚úÖ Traefik DaemonSet encontrado:"
            kubectl get daemonset traefik -n kube-system
            echo ""
            echo "Pods do Traefik:"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik
            READY_PODS=$(kubectl get daemonset traefik -n kube-system -o jsonpath='{.status.numberReady}' 2>/dev/null || echo "0")
            DESIRED_PODS=$(kubectl get daemonset traefik -n kube-system -o jsonpath='{.status.desiredNumberScheduled}' 2>/dev/null || echo "0")
            if [ "$READY_PODS" = "$DESIRED_PODS" ] && [ "$READY_PODS" != "0" ]; then
              echo "‚úÖ Traefik est√° RODANDO ($READY_PODS/$DESIRED_PODS pods prontos)"
            else
              echo "‚ö†Ô∏è  Traefik N√ÉO est√° totalmente pronto ($READY_PODS/$DESIRED_PODS pods)"
            fi
          else
            echo "‚ùå Traefik N√ÉO encontrado no cluster!"
            echo "Verifique se o Traefik est√° instalado:"
            echo "  kubectl get all -n kube-system | grep traefik"
            exit 1
          fi
          
          echo ""
          echo "=== 2. NODEPORT DO SERVICE ==="
          TRAEFIK_SVC=$(kubectl get svc traefik-loadbalancer -n kube-system 2>/dev/null || echo "")
          if [ -z "$TRAEFIK_SVC" ]; then
            echo "‚ùå Service traefik-loadbalancer N√ÉO encontrado!"
            exit 1
          fi
          
          echo "Service traefik-loadbalancer:"
          kubectl get svc traefik-loadbalancer -n kube-system
          echo ""
          
          NODE_PORT=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.spec.ports[?(@.name=="http")].nodePort}' 2>/dev/null || echo "")
          if [ -z "$NODE_PORT" ]; then
            NODE_PORT=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null || echo "")
          fi
          
          if [ -z "$NODE_PORT" ]; then
            echo "‚ö†Ô∏è  NodePort n√£o encontrado no service. Verificando detalhes..."
            kubectl describe svc traefik-loadbalancer -n kube-system | grep -A 10 "Port:"
            NODE_PORT="N/A"
          else
            echo "‚úÖ NodePort atribu√≠do: $NODE_PORT"
            echo ""
            echo "‚ö†Ô∏è  IMPORTANTE: Configure o Target Group do API Gateway para usar a porta $NODE_PORT"
          fi
          
          echo ""
          echo "=== 3. STATUS DO LOAD BALANCER ==="
          NLB_HOSTNAME=$(kubectl get svc traefik-loadbalancer -n kube-system -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$NLB_HOSTNAME" ]; then
            echo "‚úÖ Traefik NLB Hostname: $NLB_HOSTNAME"
            echo ""
            echo "Use este hostname para configurar o API Gateway"
            echo "As rotas devem ser configuradas com o prefixo /api/usuarios"
            echo ""
            echo "Verifique na AWS Console (EC2 -> Load Balancers) se o NLB foi criado:"
            echo "  Nome deve come√ßar com: k8s-kubesystem-traefik"
          else
            echo "‚ùå NLB do Traefik N√ÉO foi provisionado ainda"
            echo ""
            echo "Verificando detalhes do service:"
            kubectl describe svc traefik-loadbalancer -n kube-system | tail -30
            echo ""
            echo "Verificando eventos relacionados:"
            kubectl get events -n kube-system --sort-by='.lastTimestamp' | grep -i "traefik\|loadbalancer\|nlb" | tail -10 || echo "Nenhum evento encontrado"
            echo ""
            echo "üîß POSS√çVEIS SOLU√á√ïES:"
            echo "1. Verifique permiss√µes IAM do cluster EKS:"
            echo "   - O cluster precisa de permiss√µes para criar Load Balancers"
            echo "   - Verifique a role do node group ou IRSA"
            echo ""
            echo "2. Verifique tags das sub-redes:"
            echo "   - Sub-redes p√∫blicas devem ter: kubernetes.io/role/elb=1"
            echo "   - Sub-redes privadas devem ter: kubernetes.io/role/internal-elb=1"
            echo ""
            echo "3. Verifique se h√° limite de Load Balancers na conta AWS"
            echo ""
            echo "4. Verifique logs do AWS Load Balancer Controller (se instalado):"
            echo "   kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller"
          fi
          
          echo ""
          echo "=== 4. STATUS DO SERVICE USUARIOS ==="
          kubectl get svc usuarios-service -n users
          
          echo ""
          echo "=== 5. STATUS DO MIDDLEWARE ==="
          kubectl get middleware -A
          kubectl get middleware -n users 2>/dev/null || echo "Middleware n√£o encontrado ou CRD n√£o instalado"
          if kubectl get middleware usuarios-strip -n users >/dev/null 2>&1; then
            echo "‚úÖ Middleware usuarios-strip encontrado:"
            kubectl get middleware usuarios-strip -n users -o yaml
          else
            echo "‚ùå Middleware usuarios-strip N√ÉO encontrado!"
          fi
          
          echo ""
          echo "=== 6. VERIFICA√á√ÉO DO INGRESS ==="
          INGRESS=$(kubectl get ingress usuarios-ingress -n users 2>/dev/null || echo "")
          if [ -n "$INGRESS" ]; then
            echo "‚úÖ Ingress encontrado:"
            kubectl get ingress usuarios-ingress -n users
            echo ""
            echo "Detalhes do Ingress:"
            kubectl describe ingress usuarios-ingress -n users
          else
            echo "‚ùå Ingress n√£o encontrado!"
          fi
          
          echo ""
          echo "=== 7. TESTE DE CONECTIVIDADE ==="
          echo "Verificando se os pods do usuarios-service est√£o prontos..."
          kubectl get pods -n users -l app=usuarios
          
          echo ""
          echo "Testando conectividade direta no service (port-forward)..."
          echo "Para testar manualmente:"
          echo "  kubectl port-forward -n users svc/usuarios-service 8080:80"
          echo "  curl http://localhost:8080/"
          
          echo ""
          echo "=== 8. LOGS DO TRAEFIK (√∫ltimas 20 linhas) ==="
          TRAEFIK_POD=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$TRAEFIK_POD" ]; then
            echo "Logs do pod Traefik: $TRAEFIK_POD"
            kubectl logs -n kube-system $TRAEFIK_POD --tail=20 | grep -i "usuarios\|ingress\|middleware" || echo "Nenhum log relevante encontrado"
          else
            echo "‚ö†Ô∏è  Pod do Traefik n√£o encontrado"
          fi
